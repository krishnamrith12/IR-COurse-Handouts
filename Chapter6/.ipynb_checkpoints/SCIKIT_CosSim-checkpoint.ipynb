{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Term Document Matrices with standard libraries.\n",
    "\n",
    "We introduce the standard library Scikit Learn, for creating Term document matrices, and computing the cosine simialrity between documents and queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading necessary ibrary. Execute once and forget about this\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of docuemnts -  208\n"
     ]
    }
   ],
   "source": [
    "#Loading dataset - Our dataset is collection of 208 documents. Each document is an episode from How I met your mother.\n",
    "\n",
    "docs = eval(open('eps.txt').read())\n",
    "print 'Total number of docuemnts - ',len(docs)\n",
    "\n",
    "# eps is a list, with ep1 is a string which contains all sentences in episode 1. [ep1, ep2, ep3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Term-document matrix for word counts.\n",
    "\n",
    "The CountVectorzier provides different options, while creating the term document matrix. For full [documentation](http://www.scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) check - [scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html](http://www.scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Vcount = CountVectorizer(analyzer='word', ngram_range=(1,1), stop_words = 'english')\n",
    "countMatrix = Vcount.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'lockbox', u'locked', u'locker', u'locket', u'locking', u'lockroom', u'locks', u'locksmith', u'loco', u'locusts', u'lodge', u'lodged', u'lodging', u'lofty', u'log', u'logarithms', u'logic', u'logically', u'logistical', u'logistics']\n",
      "18972\n",
      "  (0, 10681)\t1\n",
      "  (0, 16809)\t1\n",
      "met 1\n",
      "ted 1\n"
     ]
    }
   ],
   "source": [
    "feature_names = Vcount.get_feature_names() # Feature names is the vocabbulary, i.e. all unique words (also bigrams and trigrams)\n",
    "\n",
    "print feature_names[10000:10020] #20 of the words\n",
    "\n",
    "print len(feature_names)\n",
    "\n",
    "query = [\"have you met Ted?\"]   #Query for which we have to find the most relevant documents from the dataset\n",
    "\n",
    "newCounter = Vcount.transform(query)  # Not fit_transform. Why?\n",
    "\n",
    "#Note the change in representation at the output. We can find, that the library uses a sparse matrix representation.\n",
    "#Do you think its advantageous to use a sparse representation? Why?\n",
    "\n",
    "print newCounter\n",
    "\n",
    "for item in newCounter.indices:\n",
    "    print feature_names[item],newCounter[0,item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46982511  0.49490873  0.39776136  0.4080457   0.34918273  0.35217454\n",
      "   0.45673164  0.39605804  0.3432839   0.57498765  0.3743505   0.48402476\n",
      "   0.53727265  0.25685235  0.22027905  0.33133984  0.27294293  0.47338797\n",
      "   0.44436048  0.20347623  0.44959571  0.42012999  0.42888626  0.21748043\n",
      "   0.54513427  0.39208843  0.20109782  0.41930449  0.41930449  0.25832885\n",
      "   0.37893153  0.17787032  0.48214366  0.35809444  0.39993708  0.33424685\n",
      "   0.38547167  0.43573326  0.38956891  0.2409969   0.03082949  0.23641635\n",
      "   0.16695486  0.41883081  0.45193151  0.27590232  0.49527825  0.36807145\n",
      "   0.49022195  0.3265684   0.24629266  0.08973091  0.38857077  0.03464015\n",
      "   0.26533928  0.16245673  0.52939212  0.16327821  0.1991428   0.1954712\n",
      "   0.30918883  0.4381909   0.2822562   0.16645274  0.31591373  0.22198725\n",
      "   0.38992356  0.39739954  0.4727522   0.43893594  0.30678827  0.28466895\n",
      "   0.40460185  0.44730945  0.33989048  0.37179893  0.37430613  0.19912918\n",
      "   0.1710311   0.33770107  0.41647925  0.37642615  0.31062046  0.29734053\n",
      "   0.36888452  0.44020847  0.4672446   0.33477101  0.33560479  0.38576781\n",
      "   0.37474317  0.25611449  0.38806083  0.2982052   0.30532185  0.27812708\n",
      "   0.28014723  0.43284021  0.2992405   0.36954119  0.33213591  0.35113767\n",
      "   0.37212965  0.44834641  0.22815276  0.33017132  0.25954862  0.47129089\n",
      "   0.40494779  0.3802661   0.47828912  0.47828912  0.37420811  0.25614667\n",
      "   0.34930809  0.27542724  0.43021233  0.20179898  0.30755501  0.33329219\n",
      "   0.25466287  0.40740387  0.32622937  0.37043113  0.19562701  0.25075661\n",
      "   0.33709285  0.29107507  0.36449436  0.19302191  0.16458974  0.32014981\n",
      "   0.20731367  0.27784128  0.39293412  0.36808546  0.36048607  0.29793135\n",
      "   0.29793135  0.10179078  0.05973966  0.17031008  0.14031868  0.13111051\n",
      "   0.1095964   0.15550792  0.09580604  0.09723478  0.04638786  0.09468505\n",
      "   0.09488293  0.12431983  0.20456101  0.12430824  0.14654988  0.11028067\n",
      "   0.1505284   0.12743019  0.11691048  0.12737754  0.11586553  0.04763417\n",
      "   0.07695154  0.10927843  0.14876836  0.05706353  0.11395303  0.03932545\n",
      "   0.18423423  0.1860229   0.17005744  0.17003847  0.14315502  0.16768232\n",
      "   0.17662497  0.11582596  0.19877675  0.16336872  0.09417201  0.2132438\n",
      "   0.11945462  0.08504438  0.09780193  0.09510639  0.08983075  0.08990715\n",
      "   0.12553504  0.17801077  0.13663762  0.24598429  0.12744903  0.05224463\n",
      "   0.18792487  0.06176547  0.24301015  0.02454568  0.09673017  0.00717588\n",
      "   0.10314862  0.09455023  0.07471743  0.08192869  0.10726881  0.09968208\n",
      "   0.10812826  0.09774528  0.17937529  0.17927293]]\n",
      "\n",
      "\n",
      "The index numbers for top5 results are [ 9 24 12 56 46]\n",
      "\n",
      "Episode Number, Cosine Similarity\n",
      "Episode 10 0.574987648089\n",
      "Episode 25 0.545134266785\n",
      "Episode 13 0.537272652739\n",
      "Episode 57 0.529392115934\n",
      "Episode 47 0.49527825154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosMat = cosine_similarity(newCounter,countMatrix)\n",
    "print cosMat\n",
    "\n",
    "#Finding the top 5 results\n",
    "related_docs_indices = cosMat[0].argsort()[:-6:-1]\n",
    "\n",
    "\n",
    "print '\\n\\nThe index numbers for top5 results are', related_docs_indices\n",
    "\n",
    "print '\\nEpisode Number, Cosine Similarity'\n",
    "for item in related_docs_indices:\n",
    "    print 'Episode', item+1, cosMat[0][item]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Term-document matrix for tf-idf values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'apped', u'appelle', u'appendicitis', u'appetite', u'appetites', u'appetizer', u'appetizers', u'applaud', u'applauded', u'applauding']\n",
      "  (0, 16809)\t0.600384336013\n",
      "  (0, 10681)\t0.799711603686 \n",
      "\n",
      "ted 0.600384336013\n",
      "met 0.799711603686\n"
     ]
    }
   ],
   "source": [
    "Vtfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), stop_words = 'english')\n",
    "tfidfMatrix = Vtfidf.fit_transform(docs)\n",
    "\n",
    "tf_feature_names = Vcount.get_feature_names() \n",
    "print feature_names[1000:1010]\n",
    "\n",
    "query = [\"have you met Ted?\"]\n",
    "\n",
    "newtfidf = Vtfidf.transform(query)\n",
    "\n",
    "print newtfidf,'\\n'\n",
    "\n",
    "for item in newtfidf.indices:\n",
    "    print feature_names[item],newtfidf[0,item]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.29832037  0.31381361  0.22427591  0.16382004  0.16869673  0.18560001\n",
      "   0.19852501  0.25572387  0.14795766  0.36719511  0.1483884   0.2141065\n",
      "   0.26921911  0.17062134  0.10013598  0.1479135   0.15012545  0.30781399\n",
      "   0.19952338  0.12073193  0.28791861  0.22473619  0.2939716   0.15485519\n",
      "   0.38643182  0.24922804  0.09811546  0.17076153  0.17076153  0.16169259\n",
      "   0.24786607  0.08206474  0.28162381  0.18044557  0.1865094   0.16960901\n",
      "   0.22120904  0.28781123  0.22800576  0.12546763  0.01554137  0.12567691\n",
      "   0.09328413  0.29383486  0.21320968  0.13038106  0.23258037  0.18986805\n",
      "   0.24560645  0.1597784   0.15753429  0.04081194  0.21887841  0.0149117\n",
      "   0.13497876  0.08644725  0.23433097  0.10485147  0.10050163  0.0854796\n",
      "   0.1924596   0.19810858  0.11859001  0.077826    0.19036977  0.083194\n",
      "   0.1933294   0.23956284  0.23063843  0.26001025  0.1997866   0.12382375\n",
      "   0.19262909  0.2182133   0.15278339  0.23708279  0.22434157  0.11373675\n",
      "   0.07323738  0.15990011  0.23315832  0.19131975  0.16054551  0.11401434\n",
      "   0.18226194  0.21491442  0.20663334  0.20902141  0.20404789  0.12665597\n",
      "   0.24180264  0.1552279   0.22022687  0.18841595  0.18734835  0.1284089\n",
      "   0.14299718  0.14360738  0.16870296  0.15838284  0.12538861  0.16901994\n",
      "   0.20297846  0.17206796  0.12071327  0.20164877  0.13508888  0.27960852\n",
      "   0.28434171  0.15288868  0.18564677  0.18564677  0.24084193  0.10119349\n",
      "   0.21212561  0.14255401  0.22432973  0.1072981   0.09792204  0.16283003\n",
      "   0.1063517   0.14158882  0.16882565  0.20892571  0.08894575  0.12273629\n",
      "   0.14632228  0.13308416  0.18800662  0.08969551  0.07333577  0.17929729\n",
      "   0.079473    0.14742726  0.16646546  0.19777602  0.19401839  0.13385822\n",
      "   0.13385822  0.05171977  0.02693     0.07579323  0.065457    0.06676559\n",
      "   0.04858359  0.07126249  0.05112657  0.04708647  0.01959314  0.04578181\n",
      "   0.037582    0.06390618  0.11155868  0.06693188  0.07058406  0.05682641\n",
      "   0.07563569  0.06043922  0.06091122  0.05232593  0.06348068  0.01942642\n",
      "   0.04047479  0.04932943  0.07117883  0.02331444  0.0470481   0.01486788\n",
      "   0.08450386  0.08144412  0.08214382  0.08213638  0.07968313  0.07955809\n",
      "   0.07235075  0.05948713  0.08196111  0.0767272   0.03873093  0.08989629\n",
      "   0.06935374  0.03705426  0.04163826  0.04934765  0.0514443   0.05157913\n",
      "   0.05502403  0.08520491  0.0574013   0.12797268  0.05456442  0.02222775\n",
      "   0.09903109  0.02917128  0.09925391  0.01223675  0.05033555  0.00438461\n",
      "   0.05222234  0.07308428  0.04741428  0.03536666  0.05507204  0.05300816\n",
      "   0.04052777  0.04266952  0.12424921  0.1242149 ]]\n",
      "[24  9  1 17  0]\n",
      "[ 0.38643182  0.36719511  0.31381361  0.30781399  0.29832037]\n",
      "Episode 25 0.545134266785\n",
      "Episode 10 0.574987648089\n",
      "Episode 2 0.494908727406\n",
      "Episode 18 0.473387974178\n",
      "Episode 1 0.469825111388\n"
     ]
    }
   ],
   "source": [
    "cosMattf = cosine_similarity(newtfidf,tfidfMatrix)\n",
    "print cosMattf\n",
    "related_docs_indices = cosMattf[0].argsort()[:-6:-1]\n",
    "print related_docs_indices\n",
    "print cosMattf[0][related_docs_indices]\n",
    "\n",
    "for item in related_docs_indices:\n",
    "    print 'Episode', item+1, cosMat[0][item]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "- Compare the results for different queries, when we use word-counts an when we use tf-idf \n",
    "    - \"Actually, I think it's cute\"\n",
    "    - \"So do you think you'll ever get married?\"\n",
    "- What happens, when we consider bigrams and trigrams in the term docuemnt matrix. (Check the argument in the vectorizer objects)\n",
    "- Try keeping a threshold on minimum document frequency each word should have and how that affects the queries (Check the docuemntation link)\n",
    "\n",
    "- What happens when we use euclidean distance, instead of cosine similarity? Try to apply different pairwise comaprison [metrics](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) and see the results\n",
    "- Try to apply various preprocessing steps we have done in previous sheet and compare the results with each modification. Is there any preprocessing step that we are missing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "name": "SCIKIT.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
