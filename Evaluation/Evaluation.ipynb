{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of search query results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset - documents in which the search needs to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cranInp = open('cran.all.1400').read().replace('.T\\n','\\n').replace('.A\\n','\\n').replace('.B\\n','\\n').replace('.W\\n','\\n').split('\\n.I ')\n",
    "\n",
    "# Figure out what is happening here. You have already seen this.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "Vcount = TfidfVectorizer(analyzer='word', ngram_range=(1,1), stop_words = 'english')\n",
    "countMatrix = Vcount.fit_transform(cranInp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loading the sample queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cranQuery = open('cran.qry').read().replace('.W\\r','').split('.I ')[1:]\n",
    "\n",
    "cranQuery[0]\n",
    "queryDict = dict()\n",
    "queryVects = dict()\n",
    "\n",
    "for item in cranQuery:\n",
    "    stuff = item.split('\\r\\n\\n')\n",
    "    queryDict[stuff[0]] = stuff[1].strip('\\r\\n').replace('\\r',' ')\n",
    "    queryVects[stuff[0]] = Vcount.transform([stuff[1].strip('\\r\\n').replace('\\r',' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Loading the Query relevance Judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "queryRel = open('cranqrel').read().split('\\n')\n",
    "\n",
    "queryRelDict = defaultdict(dict)\n",
    "for item in queryRel:\n",
    "    stuff = item.split()\n",
    "    try:\n",
    "        queryRelDict[stuff[0]][stuff[2]].append(stuff[1])\n",
    "    except:\n",
    "        queryRelDict[stuff[0]][stuff[2]] = list()\n",
    "        queryRelDict[stuff[0]][stuff[2]].append(stuff[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 13 0.330903219959\n",
      "Document 184 0.287073337305\n",
      "Document 12 0.22903580906\n",
      "Document 875 0.19808803912\n",
      "Document 486 0.197586554438\n",
      "Document 51 0.182630719804\n",
      "Document 746 0.177758978159\n",
      "Document 1268 0.151197563192\n",
      "Document 327 0.144169282408\n",
      "Document 792 0.136523347919\n"
     ]
    }
   ],
   "source": [
    "# Query 1: \"what similarity laws must be obeyed when constructing aeroelastic models of heated high speed aircraft\"\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosMattf = cosine_similarity(queryVects['001'],countMatrix)\n",
    "related_docs_indices = cosMattf[0].argsort()[:-11:-1]\n",
    "\n",
    "\n",
    "for item in related_docs_indices:\n",
    "    print 'Document', item+1, cosMattf[0][item]\n",
    "\n",
    "tp = list()\n",
    "for item in queryRelDict['1'].keys():\n",
    "    for stuff in related_docs_indices:\n",
    "        if str(stuff+1)in queryRelDict['1'][item]:\n",
    "            tp.append(stuff+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 51, 184, 875, 486, 13]\n",
      "Precision is 0.6\n"
     ]
    }
   ],
   "source": [
    "print tp\n",
    "\n",
    "#All other entries which are in related_docs_indices but not in queryRelDict['1'] are false positives\n",
    "precision = 1.0*len(tp)/len(related_docs_indices)\n",
    "\n",
    "print 'Precision is', precision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "recall is 0.206896551724\n"
     ]
    }
   ],
   "source": [
    "recallDocLen = 0\n",
    "for item in queryRelDict['1'].keys():\n",
    "    recallDocLen += len(queryRelDict['1'][item])\n",
    "    \n",
    "print recallDocLen\n",
    "\n",
    "recall = 1.0*len(tp)/recallDocLen\n",
    "\n",
    "print 'recall is', recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall @ K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision is 0.333333333333\n",
      "Recall is 0.344827586207\n"
     ]
    }
   ],
   "source": [
    "# change the value for k\n",
    "k = 30\n",
    "\n",
    "cosMattf = cosine_similarity(queryVects['001'],countMatrix)\n",
    "related_docs_indices = cosMattf[0].argsort()[:-1*(k+1):-1]\n",
    "\n",
    "tp = list()\n",
    "for item in queryRelDict['1'].keys():\n",
    "    for stuff in related_docs_indices:\n",
    "        if str(stuff+1)in queryRelDict['1'][item]:\n",
    "            tp.append(stuff+1)\n",
    "            \n",
    "precision = 1.0*len(tp)/len(related_docs_indices)\n",
    "recall = 1.0*len(tp)/recallDocLen\n",
    "\n",
    "print 'Precision is', precision\n",
    "print 'Recall is', recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0]\n",
      "P@ 1  :  1.0\n",
      "P@ 2  :  1.0\n",
      "P@ 3  :  1.0\n",
      "P@ 4  :  1.0\n",
      "P@ 5  :  1.0\n",
      "P@ 6  :  1.0\n",
      "P@ 24  :  0.291666666667\n",
      "P@ 26  :  0.307692307692\n",
      "P@ 28  :  0.321428571429\n",
      "P@ 29  :  0.344827586207\n",
      "Average Prcision @ K for query 1 : 0.726561513199\n"
     ]
    }
   ],
   "source": [
    "#Average Precision @ Key\n",
    "\n",
    "relOrNot = [0]*k\n",
    "for item in queryRelDict['1'].keys():\n",
    "    for i in range(len(related_docs_indices)):\n",
    "        if str(related_docs_indices[i]+1) in queryRelDict['1'][item]:\n",
    "            relOrNot[i] = 1       \n",
    "            \n",
    "print relOrNot\n",
    "avgPs = list()\n",
    "\n",
    "for i in range(len(relOrNot)):\n",
    "    if relOrNot[i] == 1:\n",
    "        print 'P@',i+1,' : ',sum(relOrNot[:i+1])*1.0/(i+1)\n",
    "        avgPs.append(sum(relOrNot[:i+1])*1.0/(i+1))\n",
    "\n",
    "        \n",
    "print 'Average Prcision @ K for query 1 :', sum(avgPs)/len(avgPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "MAP - Mean Average Precision is nothing but Average Precision across multiple queries for the document collection. Queries are stored in **queryVects** in vectorial forms. Keys being '001','002','028' etc.\n",
    "\n",
    " - With the above code for average precision, find MAP.\n",
    " - try to fill **relOrNot** with list comprehension. [Hint](http://stackoverflow.com/questions/35332878/python-list-comprehension-when-item-index-is-required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DCGrel = [0]*k\n",
    "for item in queryRelDict['1'].keys():\n",
    "    for i in range(len(related_docs_indices)):\n",
    "        if str(related_docs_indices[i]+1) in queryRelDict['1'][item]:\n",
    "            DCGrel[i] = 5 - int(item)       # In DCG higher value means more relevant. But in the dataset, the relevance is ranked.\n",
    "                                            # In the Dataset, 5 is not relevant and 1 is most relevant.\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "DCG = [int(DCGrel[0])]\n",
    "summedDCG = [DCG[0]]\n",
    "\n",
    "\n",
    "for i,item in enumerate(DCGrel[1:]):\n",
    "    DCG.append(int(item)*1.0/math.log(i+2))\n",
    "    summedDCG.append(sum(DCG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG @ Rank  1  :  1\n",
      "DCG @ Rank  2  :  5.32808512267\n",
      "DCG @ Rank  3  :  7.14856357592\n",
      "DCG @ Rank  4  :  9.31260613725\n",
      "DCG @ Rank  5  :  13.0406157446\n",
      "DCG @ Rank  6  :  14.1568369977\n",
      "DCG @ Rank  7  :  14.1568369977\n",
      "DCG @ Rank  8  :  14.1568369977\n",
      "DCG @ Rank  9  :  14.1568369977\n",
      "DCG @ Rank  10  :  14.1568369977\n",
      "DCG @ Rank  11  :  14.1568369977\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(summedDCG[:11]):\n",
    "    print 'DCG @ Rank ',i+1,' : ',item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    " - With the above code for DCG, find NDCG. [Hint](https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain)\n",
    " - Find log to the base of 2. Read documentation for math.log\n",
    " - With the code for 'Precision and Recall @ K' section, and from the plotting function introduced in previous notebook, plot the precision-recall curve. (hint - Find PR at each value of k and store them in separate lists to plot.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (SageMath)",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "2016-02-08-153225.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
